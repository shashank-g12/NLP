Group members: Swaransh Patel(2022AIB2678) and Shashank G(2022AIB2684)

Tried bart-base, bart-large and t5-small for this sequence to sequence task and found that t5-small gave the best accuracy on dev set. The best result obtained is as below:

"accuracy": 0.8782355478861087,
"intent_accuracy": 0.988244176013805,
"parsing_accuracy": 1.0

We tried different techniques to choose the right information to give as input. We decided to these techniques for final model:
i) using regular expression to clean the input text
ii) using user_contact information
ii) using the most recent history query and response information

We also performed hyper-parameter tuning. Used a batch size of 64 and learning rate of 2e-4.
The fine tuned-model is uploaded in the directory as mentioned in the submission instruction which is named as "t5-acc-2.pth".
